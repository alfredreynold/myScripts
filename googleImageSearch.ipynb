{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import urllib2\n",
    "import mechanize\n",
    "import BeautifulSoup\n",
    "import codecs\n",
    "import csv,xlrd,json\n",
    "from urlparse import urlparse\n",
    "from pprint import pprint\n",
    "import gspread,os\n",
    "from oauth2client.client import SignedJwtAssertionCredentials\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from PIL import Image\n",
    "from StringIO import StringIO\n",
    "from requests.exceptions import ConnectionError\n",
    "import urllib\n",
    "import readPicFileAsDictionary as picDic\n",
    "from bingSearch import BingSearch\n",
    "import sqlite3\n",
    "\n",
    "def go(query,lang):\n",
    "    \"\"\"Download full size images from Google image search.\n",
    "    Don't print or republish images without permission.\n",
    "    I used this to train a learning algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"https://developers.google.com/image-search/v1/jsondevguide#json_args\"\"\"\n",
    "    \n",
    "    BASE_URL = 'https://ajax.googleapis.com/ajax/services/search/images?'\\\n",
    "             'v=1.0&q=' + query + '&start=%d&as_rights=cc_publicdomain'\n",
    "\n",
    "    urls = []\n",
    "    start = 0 # Google's start query string parameter for pagination.\n",
    "    while start < 60: # Google will only return a max of 56 results.\n",
    "        r = requests.get(BASE_URL %start)\n",
    "        print (BASE_URL %start)\n",
    "        \n",
    "        js = json.loads(r.text)\n",
    "        if(js[\"responseStatus\"] != 200):\n",
    "            print js[\"responseDetails\"]\n",
    "        if r and js:\n",
    "            resData = js['responseData']\n",
    "            if resData and resData['results']:\n",
    "                for image_info in resData['results']:\n",
    "                    url = image_info['unescapedUrl']\n",
    "                    if \"pixabay\" not in url and \"fotocommunity\" not in url and \".svg\" not in url and \".gif\" not in url:                        \n",
    "                        if len(urls) > 9:\n",
    "                            break\n",
    "                        print url\n",
    "                        urls.append(url)\n",
    "                        \n",
    "            start += 4 # 4 images per page.\n",
    "        if len(urls) > 10:\n",
    "            break\n",
    "        # Be nice to Google and they'll be nice back :)\n",
    "        time.sleep(10)\n",
    "        \n",
    "    return urls\n",
    "    \n",
    "\n",
    "def getPics(searchURL):\n",
    "    img_urls = []\n",
    "    browser = mechanize.Browser()\n",
    "    browser.set_handle_robots(False)\n",
    "    browser.addheaders = [('User-agent',\n",
    "                           'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.114 Safari/537.36')]\n",
    "    browser.set_proxies({\"http\": \"10.200.1.26:8080\", \"https\": \"10.200.1.26:8080\"})\n",
    "    try:\n",
    "        print searchURL.encode('utf-8')\n",
    "        res = browser.open(searchURL.encode('utf-8'))\n",
    "        htmltext = res.read()\n",
    "        soup = BeautifulSoup.BeautifulSoup(htmltext)\n",
    "        results = soup.findAll(\"a\")\n",
    "        count = 0\n",
    "        for r in results:\n",
    "            if r.get('href') != None:\n",
    "                hrf = r.get('href')\n",
    "                if (\"imgres?imgurl\" in hrf and \"pixabay\" not in hrf and \"fotocommunity\" not in hrf and \".svg\" not in hrf and \".gif\" not in hrf):\n",
    "\n",
    "                    s = hrf\n",
    "                    if(\"&imgrefurl\" in hrf):\n",
    "                        fs = s[s.find(\"imgres?imgurl=\"):s.find(\"&imgrefurl\")]\n",
    "                    else:\n",
    "                        fs = s[s.find(\"imgres?imgurl=\"):]\n",
    "                        print hrf\n",
    "\n",
    "                    print fs\n",
    "                    fs = fs.replace(\"imgres?imgurl=\", \"\")\n",
    "                    # print fs\n",
    "                    fs = urllib2.unquote(fs)\n",
    "                    fs = urllib2.unquote(fs)\n",
    "\n",
    "                    if count < 20:\n",
    "                        # print fs\n",
    "                        img_urls.append(fs)\n",
    "                        count += 1\n",
    "                    else:\n",
    "                        return img_urls\n",
    "\n",
    "        return img_urls\n",
    "\n",
    "    except Exception, arg:\n",
    "        print \"Error\" + str(arg)\n",
    "        if not len(img_urls) == 0:\n",
    "            return img_urls\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "def alfa_unicode_dict_reader(fName, fencoding):\n",
    "\n",
    "    with open(fName) as mfile:\n",
    "        # mfile = open(fName,mode=\"rb\")\n",
    "        hd = mfile.readline()\n",
    "        values = mfile.readlines()\n",
    "        # hd = values[0]\n",
    "        keys = hd.strip().split(\",\")\n",
    "        output = []\n",
    "        for row in values:\n",
    "            row = unicode(row, fencoding)\n",
    "            result = dict(zip(keys, row.strip().split(',')))\n",
    "            output.append(result)\n",
    "    return output\n",
    "\n",
    "\n",
    "def readAndWriteFile(input_file):\n",
    "    cf = alfa_unicode_dict_reader(input_file, \"utf-8\")\n",
    "    fout = codecs.open(\"ForMechTurkImgSel-g-fr.csv\", \"wb\", encoding='utf-8')\n",
    "    fout.write(\"word,category,img1,img2,img3,img4,img5,img6,img7,img8,img9,img10\\n\")\n",
    "    for rw in cf:\n",
    "        if not (rw['word'] == \"\" or rw['word'] == None):\n",
    "            search = rw['word']\n",
    "            print \"searching : \" + search\n",
    "            # search = search + \"+\" + rw['Input.category']\n",
    "            search = search.replace(\" \", \"+\")\n",
    "            types = \"Any\"\n",
    "            lang = \"fr\"\n",
    "            url = \"https://www.google.co.in/search?hl=\"+lang+\"&authuser=0&site=imghp&tbm=isch&tbs=itp:\" + types + \",isz:m,sur:fc&q=\" + search\n",
    "            # print url\n",
    "            reA = getPics(url)\n",
    "            finStr = []\n",
    "            finStr.append(rw['word'])\n",
    "            finStr.append(rw['Input.category'])\n",
    "            if (reA != None and len(reA) != 0):\n",
    "                for itm in reA:\n",
    "                    finStr.append(itm)\n",
    "\n",
    "            urStr = \",\".join([(\"\\\"\" + i + \"\\\"\") for i in finStr])\n",
    "            fout.write(urStr)\n",
    "            fout.write(\"\\n\")\n",
    "\n",
    "    fout.close()\n",
    "\n",
    "\n",
    "def escape_comma(word):\n",
    "    return word.replace(\",\",\"%2C\")\n",
    "\n",
    "\n",
    "\n",
    "def get_file_with_header(file_name):\n",
    "    fl = codecs.open(file_name, \"wb\", encoding='utf-8')\n",
    "    fl.write(\"<html>\")\n",
    "    fl.write(\"<meta http-equiv=\\\"Content-Type\\\" charset=\\\"UTF-8\\\" />\")\n",
    "    fl.write(\"<link rel=\\\"stylesheet\\\" type=\\\"text/css\\\" href=\\\"./css/alfastyle.css\\\">\")\n",
    "    fl.write(\"<head><script src=\\\"http://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js\\\"></script><script src=\\\"./js/jquery.cookie.js\\\"></script><script src=\\\"./js/alfa.js\\\"></script></head>\")\n",
    "    fl.write(\"<Body>\")\n",
    "    fl.write(\"<Form name=\\\"input\\\" action = \\\"\\\">\")\n",
    "    return fl\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_write2html(input_file):\n",
    "    oput = codecs.open(\"oput.csv\", 'w', encoding='utf8')\n",
    "\n",
    "    fl = get_file_with_header(\"results-g-fr-1.html\")\n",
    "\n",
    "    counter = 0\n",
    "    file_counter = 1\n",
    "\n",
    "    cf = alfa_unicode_dict_reader(input_file, \"utf-8\")\n",
    "    nmatch = 0\n",
    "\n",
    "    # print cf\n",
    "    for i in range(len(cf)):\n",
    "        rw = cf[i]\n",
    "        if not (rw['word'] == \"\" or rw['word'] == None):\n",
    "            search = rw['word']\n",
    "            cati = rw['Input.category']\n",
    "            # print search\n",
    "            srch = \"<h2>\" + search + \" - \" + rw['Input.category'] + \"</h2>\"\n",
    "            print \"searching : \" + search\n",
    "            gsearch = search.strip(\"\\\"\").strip()\n",
    "            # gsearch = gsearch + \"+\" + cati\n",
    "            gsearch = gsearch.replace(\" \", \"+\")\n",
    "            search = search.replace(\" \", \"+\")\n",
    "            types = \"Any\"\n",
    "            lang = \"fr\"\n",
    "            url = \"https://www.google.fr/search?hl=\"+lang+\"&authuser=0&site=imghp&tbm=isch&safe=active&tbs=itp:\" + types + \",isz:m,sur:fc&q=\" + gsearch\n",
    "            # print url\n",
    "            fl.write(srch)\n",
    "            # gSrh = \"<a class = \\\"alinky\\\" href = \\\"\" + url + \"\\\">Search In Google</a></br>\"\n",
    "            fl.write(\"<div class =\\\"wrappy\\\">\")\n",
    "            # fl.write(gSrh)\n",
    "            reA = getPics(url)\n",
    "            # print reA\n",
    "            if (reA != None and len(reA) != 0):\n",
    "                for itm in reA:\n",
    "                    fl.write(\"<div class =\\\"gridy\\\">\")\n",
    "                    # print type(itm)\n",
    "                    itg = \"<a href=\\\"\" + itm + \"\\\">\" + \"<img src = \\\"\" + itm + \"\\\" class =\\\"imagy\\\" default = \\\"img\\\"></img></a>\"\n",
    "\n",
    "                    fl.write(itg)\n",
    "                    fl.write(\"</br>\")\n",
    "                    checkbox = rw['word'].strip(\"\\\"\").strip() + \",\" + rw['Input.category'].strip(\n",
    "                        \"\\\"\").strip() + \",\" + \"'\" + itm + \"'\"\n",
    "                    chbox = \"<input class =\\\"checky\\\" type=\\\"checkbox\\\" name=\\\"\" + checkbox + \"\\\">\"\n",
    "                    # print chbox\n",
    "                    fl.write(chbox)\n",
    "                    oput.write(checkbox + \"\\n\")\n",
    "                    fl.write(\"</div>\")\n",
    "\n",
    "            fl.write(\"<div class =\\\"gridy\\\">\")\n",
    "            nmatch += 1\n",
    "            fl.write(\"<h3>No Match</h3>\")\n",
    "            checkbox = rw['word'].strip(\"\\\"\").strip() + \",\" + rw['Input.category'].strip(\"\\\"\").strip() + \",\" + \"NO-MATCH-\"+str(nmatch)\n",
    "            fl.write(\"<input class =\\\"checky\\\" type=\\\"checkbox\\\" name=\\\"\" + checkbox + \"\\\">\")\n",
    "            fl.write(\"</div>\")\n",
    "            fl.write(\"</div>\")\n",
    "            fl.write(\"</br>\")\n",
    "            counter += 1\n",
    "\n",
    "        if counter == 10 or len(cf)-i<10:\n",
    "            print \"****************************\"\n",
    "            fl.write(\"<input type=\\\"submit\\\" value=\\\"Submit\\\">\")\n",
    "            fl.write(\"</Form>\")\n",
    "            fl.write(\"</Body>\")\n",
    "            fl.write(\"</html>\")\n",
    "            fl.close()\n",
    "            file_counter += 1\n",
    "            fnme = \"results-g-fr-\" + str(file_counter) + \".html\"\n",
    "            fl = get_file_with_header(fnme)\n",
    "            counter = 0\n",
    "    oput.close()\n",
    " \n",
    " \n",
    "def bing_search(query):\n",
    "    search_type = 'Image'\n",
    "    #search_type: Web, Image, News, Video\n",
    "    key= 'FM1eRnjFYoltkKdRDKcwiJMk9VFn9p/yRgru20R5xc4'\n",
    "    query = urllib.quote(query)\n",
    "    # create credential for authentication\n",
    "    user_agent = 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; FDM; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 1.1.4322)'\n",
    "    credentials = (':%s' % key).encode('base64')[:-1]\n",
    "    auth = 'Basic %s' % credentials\n",
    "    url = 'https://api.datamarket.azure.com/Data.ashx/Bing/Search/'+search_type+'?Query=%27'+query+'%27&$top=50&$format=json'\n",
    "    request = urllib2.Request(url)\n",
    "    request.add_header('Authorization', auth)\n",
    "    request.add_header('User-Agent', user_agent)\n",
    "    request_opener = urllib2.build_opener()\n",
    "    response = request_opener.open(request) \n",
    "    response_data = response.read()\n",
    "    json_result = json.loads(response_data)\n",
    "    result_list = json_result['d']['results']\n",
    "    resArray = []\n",
    "    for res in result_list:\n",
    "        url = res[u'MediaUrl']\n",
    "        if (\"wikimedia\" in url) and (\"commons\" in url):\n",
    "            if len(resArray) > 60:\n",
    "                break\n",
    "            resArray.append(url)\n",
    "            print url\n",
    "    return resArray\n",
    "\n",
    "\n",
    "\n",
    "def start(input_file,outpath,lang,picFilePath):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    input_file: The first parameter.\n",
    "    outpath: The second parameter.\n",
    "    lang: The third parameter.\n",
    "    picFilePath: The fourth parameter.\n",
    "    \"\"\"\n",
    "    data = readXL(input_file)\n",
    "    symbol1,symbol2 = picDic.readSym_Stix(lang,picFilePath)\n",
    "    print len(data)\n",
    "#     print data\n",
    "    outpath = os.path.join(outpath,\"jsons/\")\n",
    "    print outpath\n",
    "    if not os.path.exists(outpath):\n",
    "        os.makedirs(outpath)\n",
    "    print outpath\n",
    "    fout = codecs.open(os.path.join(outpath , \"json-1.json\"), \"wb\", encoding='utf-8')\n",
    "    counter = 0\n",
    "    file_counter = 1\n",
    "    nmatch = 0\n",
    "\n",
    "    fout.write(\"[\")\n",
    "    bing = BingSearch('FM1eRnjFYoltkKdRDKcwiJMk9VFn9p/yRgru20R5xc4')\n",
    "    print bing\n",
    "    for i,dd in enumerate(data):\n",
    "        rw={}\n",
    "        rw['Input.category']=dd[0]\n",
    "        rw['word']=dd[1]    \n",
    "        \n",
    "        if not (rw['word'] == \"\" or rw['word'] == None):\n",
    "            search = rw['word'].strip(\"\\\"\").strip()\n",
    "            cati = rw['Input.category'].strip(\"\\\"\").strip(\"'\").strip()\n",
    "            print \"searching : \" + search\n",
    "            types = \"Any\"\n",
    "            reA = \"\"\n",
    "            if search in symbol1:\n",
    "                reA = [os.path.join(\"png\",x) for x in symbol1[search]]\n",
    "            elif search in symbol2:\n",
    "                reA = [os.path.join(\"png\",x) for x in symbol2[search]]\n",
    "            else:                \n",
    "                reA = bing.bing_search(search.encode('utf-8'),\"Image\")\n",
    "\n",
    "            if (reA != None and len(reA) != 0):\n",
    "                lst = 0\n",
    "                counter += 1\n",
    "                fout.write(\"{\\\"\" + search + \" - \" + cati + \"\\\":[\")\n",
    "                for itm in reA:\n",
    "                    lst += 1\n",
    "                    ckbx = search + \",\" + cati + \",'\" + itm + \"'\"\n",
    "                    \n",
    "                    fout.write(\"{\\\"imgURL\\\":\\\"\" + itm + \"\\\",\\\"checkName\\\":\\\"\" + ckbx + \"\\\"}\")\n",
    "                    fout.write(\",\")\n",
    "\n",
    "                nmatch += 1\n",
    "                ckbx = search + \",\" + cati + \",\" + \"No-Match-\" + str(nmatch)\n",
    "                fout.write(\"{\\\"imgURL\\\":\\\"http://upload.wikimedia.org/wikipedia/commons/7/75/Erroricon404.PNG\\\",\\\"checkName\\\":\\\"\" + ckbx + \"\\\"}\")\n",
    "                fout.write(\"]}\")\n",
    "                if not counter > 9:\n",
    "                    fout.write(\",\")\n",
    "#             else:\n",
    "#                 nmatch += 1\n",
    "#                 ckbx = search + \",\" + cati + \",\" + \"No-Match-\" + str(nmatch)\n",
    "#                 fout.write(\"{\\\"imgURL\\\":\\\"http://upload.wikimedia.org/wikipedia/commons/7/75/Erroricon404.PNG\\\",\\\"checkName\\\":\\\"\" + ckbx + \"\\\"}\")\n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "        if counter == 10 or len(data)-i == 1:\n",
    "            print counter\n",
    "            fout.write(\"]\")\n",
    "            file_counter += 1\n",
    "            fout.close()\n",
    "            if not len(data)-i == 1:\n",
    "                fname = os.path.join(outpath,\"json-\" + str(file_counter) + \".json\")\n",
    "                fout = codecs.open(fname, \"wb\", encoding='utf-8')\n",
    "                fout.write(\"[\")\n",
    "                counter = 0\n",
    "#         else:\n",
    "#             fout.write(\",\")\n",
    "\n",
    "\n",
    "\n",
    "def readXL(path):\n",
    "    global sqlPath\n",
    "    con = sqlite3.connect(sqlPath)\n",
    "    cur = con.cursor()\n",
    "    bk = xlrd.open_workbook(path)\n",
    "    sht = bk.sheet_by_name(\"Localization\")\n",
    "    data=[]\n",
    "    for row in range(1,sht.nrows):\n",
    "        action = sht.cell_value(row,0).strip().lower()\n",
    "        avazCat = sht.cell_value(row,1).strip().upper()\n",
    "        avazCatofCat = sht.cell_value(row,2).strip().upper()\n",
    "        swedCat = sht.cell_value(row,3).strip().upper()\n",
    "        swedCatofCat = sht.cell_value(row,4).strip().upper()\n",
    "        itemsStr = sht.cell_value(row,6).strip()\n",
    "        items = [x.strip() for x in itemsStr.split(\",\")]\n",
    "        outPut = codecs.open(\"/Users/alfred/Downloads/imageSrch.csv\", 'w', encoding='utf8')\n",
    "        for item in items:\n",
    "            dbData = cur.execute(\"select * from zpicmodedict where ztag_name=?\",(item,)).fetchall()\n",
    "            if dbData:\n",
    "                continue\n",
    "            if item:\n",
    "                item = item.replace('\"',\"'\")\n",
    "                if avazCat:\n",
    "                    data.append((avazCat,item))\n",
    "                elif swedCat:\n",
    "                    data.append((swedCat,item))\n",
    "    print \"Count:\",len(data)\n",
    "    return data\n",
    "            \n",
    "   \n",
    "\n",
    "# inputXLFile = \"/Users/alfredreynold/Downloads/Avaz paÌŠ svenska - Avaz Swedish (2).xlsx\"\n",
    "# jsonOutputFolder = \"/Users/alfredreynold/ALFRED/DEV/snakes/Translation/SwedishJson\"\n",
    "# picFilePath = \"/Users/alfredreynold/ALFRED/DEV/Avaz-content-tools/symbols/picFile-latest.xlsx\"\n",
    "# sqlPath = \"/Users/alfredreynold/ALFRED/DEV/snakes/Translation/User-Data.sqlite\"\n",
    "# start(inputXLFile,jsonOutputFolder,\"sv\",picFilePath)\n",
    "# bing_search(\"coffee\")\n",
    "# go(\"alfred\",\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"Alf %s asd %s\" %(\"ret\",\"wqe\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(10):\n",
    "    print i\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bingSearch import BingSearch\n",
    "b = BingSearch('FM1eRnjFYoltkKdRDKcwiJMk9VFn9p/yRgru20R5xc4')\n",
    "print b.bing_search(\"alfred\",\"Image\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
